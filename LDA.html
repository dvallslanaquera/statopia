<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title>LDA project</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}

.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->






<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Statopia</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="about.html">About</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Models
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
    <li class="dropdown-header">Multivariant models</li>
    <li>
      <a href="LDA.html">LDA model</a>
    </li>
    <li>
      <a href="Clustering.html">Clustering models</a>
    </li>
    <li class="divider"></li>
    <li class="dropdown-header">Univariant models</li>
    <li>
      <a href="regression.html">Advanced regression models</a>
    </li>
  </ul>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">LDA project</h1>

</div>


<div id="linear-discriminant-analysis-lda" class="section level1">
<h1>Linear Discriminant Analysis (LDA)</h1>
<p>This is a method of <strong>supervised multivariant classification</strong> and it is one of the most powerful tool a data analyst have. With this method we can look for the significance of the variables and we can make predictions. To make this model we need an explanatory matrix X and a response matrix Y. We will use the iris database as example.</p>
<div class="figure">
<img src="https://www.google.co.jp/url?sa=i&amp;rct=j&amp;q=&amp;esrc=s&amp;source=images&amp;cd=&amp;cad=rja&amp;uact=8&amp;ved=2ahUKEwiEiMrDs4TcAhXZc94KHa0DA_oQjRx6BAgBEAU&amp;url=https%3A%2F%2Fwww.palass.org%2Fpublications%2Fnewsletter%2Fpalaeomath-101%2Fpalaeomath-part-10-groups-i&amp;psig=AOvVaw1xACKJnOklPvq1RMLEUeM3&amp;ust=1530757854877316" />

</div>
<table>
<caption>A caption of the Iris database</caption>
<thead>
<tr class="header">
<th align="right">Sepal.Length</th>
<th align="right">Sepal.Width</th>
<th align="right">Petal.Length</th>
<th align="right">Petal.Width</th>
<th align="left">Species</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">5.1</td>
<td align="right">3.5</td>
<td align="right">1.4</td>
<td align="right">0.2</td>
<td align="left">setosa</td>
</tr>
<tr class="even">
<td align="right">4.9</td>
<td align="right">3.0</td>
<td align="right">1.4</td>
<td align="right">0.2</td>
<td align="left">setosa</td>
</tr>
<tr class="odd">
<td align="right">4.7</td>
<td align="right">3.2</td>
<td align="right">1.3</td>
<td align="right">0.2</td>
<td align="left">setosa</td>
</tr>
<tr class="even">
<td align="right">4.6</td>
<td align="right">3.1</td>
<td align="right">1.5</td>
<td align="right">0.2</td>
<td align="left">setosa</td>
</tr>
<tr class="odd">
<td align="right">5.0</td>
<td align="right">3.6</td>
<td align="right">1.4</td>
<td align="right">0.2</td>
<td align="left">setosa</td>
</tr>
</tbody>
</table>
<div id="standarization-and-exploration" class="section level2">
<h2>1- Standarization and exploration</h2>
<pre class="r"><code>iris.hle &lt;- decostand(as.matrix(iris[1:4]), &quot;hellinger&quot;) 
gr &lt;- cutree(hclust(vegdist(iris.hle, &quot;euc&quot;), &quot;ward.D&quot;), 3)
table(gr)</code></pre>
<pre><code>## gr
##  1  2  3 
## 50 48 52</code></pre>
<p>We used k = 3 groups, as the theory suggests. The model classified the data in 3 gropus of similar size.</p>
</div>
<div id="assumptions-check" class="section level2">
<h2>2- Assumptions check</h2>
<p>The LDA model is a parametric model, so there are assumptions to check and control.</p>
<div id="na-values" class="section level3">
<h3>2.1. NA values</h3>
<pre class="r"><code>any(is.na(iris))</code></pre>
<pre><code>## [1] FALSE</code></pre>
<p>There are not any NA value.</p>
</div>
<div id="multivariant-homogeneity" class="section level3">
<h3>2.2. Multivariant homogeneity</h3>
<pre class="r"><code>iris.pars &lt;- as.matrix(iris[, 1:4])
iris.pars.d &lt;- dist(iris.pars)
(iris.MHV &lt;- betadisper(iris.pars.d, gr))</code></pre>
<pre><code>## 
##  Homogeneity of multivariate dispersions
## 
## Call: betadisper(d = iris.pars.d, group = gr)
## 
## No. of Positive Eigenvalues: 4
## No. of Negative Eigenvalues: 0
## 
## Average distance to median:
##      1      2      3 
## 0.4814 0.6990 0.8190 
## 
## Eigenvalues for PCoA axes:
##    PCoA1    PCoA2    PCoA3    PCoA4     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt; 
## 630.0080  36.1579  11.6532   3.5514       NA       NA       NA       NA</code></pre>
<pre class="r"><code>anova(iris.MHV)</code></pre>
<pre><code>## Analysis of Variance Table
## 
## Response: Distances
##            Df  Sum Sq Mean Sq F value    Pr(&gt;F)    
## Groups      2  2.9735  1.4867  10.884 3.909e-05 ***
## Residuals 147 20.0803  0.1366                      
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<pre class="r"><code>permutest(iris.MHV)</code></pre>
<pre><code>## 
## Permutation test for homogeneity of multivariate dispersions
## Permutation: free
## Number of permutations: 999
## 
## Response: Distances
##            Df  Sum Sq Mean Sq      F N.Perm Pr(&gt;F)    
## Groups      2  2.9735  1.4867 10.884    999  0.001 ***
## Residuals 147 20.0803  0.1366                         
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>We can not accept the homogeneity assumption. We try to transform the data eliminating outlier values.</p>
<p><img src="LDA_files/figure-html/unnamed-chunk-5-1.png" width="672" /></p>
<pre class="r"><code>skewness(iris[1:4])</code></pre>
<pre><code>## Sepal.Length  Sepal.Width Petal.Length  Petal.Width 
##    0.3086407    0.3126147   -0.2694109   -0.1009166</code></pre>
<pre class="r"><code>outliers::outlier(iris[2])</code></pre>
<pre><code>## Sepal.Width 
##         4.4</code></pre>
<pre class="r"><code>which(iris[2] &gt;= 4.1 | iris[2] &lt; 2.2) </code></pre>
<pre><code>## [1] 16 33 34 61</code></pre>
<pre class="r"><code>iris[c(16, 33, 34, 61), 2] &lt;- mean(iris[,2])

iris2 &lt;- cbind(log(iris[1]), iris[2], iris[3], iris[4])
skewness(iris2[1:4])</code></pre>
<pre><code>## Sepal.Length  Sepal.Width Petal.Length  Petal.Width 
##   0.04272597   0.12722662  -0.26941093  -0.10091657</code></pre>
<p><img src="LDA_files/figure-html/unnamed-chunk-8-1.png" width="672" /></p>
<p>The two first variables are skewed to the right and the third one is skewed to the left.</p>
<pre class="r"><code>iris.pars2 &lt;- as.matrix(iris2)
iris.pars.d2 &lt;- dist(iris.pars2)
(iris.MHV2 &lt;- betadisper(iris.pars.d2, gr))</code></pre>
<pre><code>## 
##  Homogeneity of multivariate dispersions
## 
## Call: betadisper(d = iris.pars.d2, group = gr)
## 
## No. of Positive Eigenvalues: 4
## No. of Negative Eigenvalues: 0
## 
## Average distance to median:
##      1      2      3 
## 0.3401 0.5081 0.6266 
## 
## Eigenvalues for PCoA axes:
##    PCoA1    PCoA2    PCoA3    PCoA4     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt;     &lt;NA&gt; 
## 551.4433  19.7877   5.1255   0.4618       NA       NA       NA       NA</code></pre>
<pre class="r"><code>anova(iris.MHV2)</code></pre>
<pre><code>## Analysis of Variance Table
## 
## Response: Distances
##            Df  Sum Sq Mean Sq F value    Pr(&gt;F)    
## Groups      2  2.1079 1.05394  14.288 2.137e-06 ***
## Residuals 147 10.8436 0.07377                      
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<pre class="r"><code>permutest(iris.MHV2)</code></pre>
<pre><code>## 
## Permutation test for homogeneity of multivariate dispersions
## Permutation: free
## Number of permutations: 999
## 
## Response: Distances
##            Df  Sum Sq Mean Sq      F N.Perm Pr(&gt;F)    
## Groups      2  2.1079 1.05394 14.288    999  0.001 ***
## Residuals 147 10.8436 0.07377                         
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>We can not accept the homogeneity of the sample even after transforming the data. In this case we should make a non-parametric model like a Quadratic Discriminant Analysis (QDA).</p>
</div>
<div id="normality" class="section level3">
<h3>2.3. Normality</h3>
<pre class="r"><code>par(mfrow = c(1, ncol(iris.pars2)))
for(j in 1:ncol(iris.pars2)){
  hist(iris.pars2[,j])}</code></pre>
<p><img src="LDA_files/figure-html/unnamed-chunk-10-1.png" width="672" /></p>
<pre class="r"><code>mshapiro.test(t(iris.pars2))</code></pre>
<pre><code>## 
##  Shapiro-Wilk normality test
## 
## data:  Z
## W = 0.97503, p-value = 0.007804</code></pre>
<p>We can not accept the normality assumption too, due to the third and forth variables anormality.</p>
</div>
<div id="multicollinearity" class="section level3">
<h3>2.4. Multicollinearity</h3>
<pre class="r"><code>as.dist(cor(iris.pars2))</code></pre>
<pre><code>##              Sepal.Length Sepal.Width Petal.Length
## Sepal.Width    -0.1377389                         
## Petal.Length    0.8786347  -0.4001647             
## Petal.Width     0.8281772  -0.3359133    0.9628654</code></pre>
<pre class="r"><code>faraway::vif(iris.pars2)</code></pre>
<pre><code>## Sepal.Length  Sepal.Width Petal.Length  Petal.Width 
##     6.236970     1.729705    27.477004    15.465980</code></pre>
<p>There is one problem of multicollinearity between Petal.Length - Petal.Width. We will continue our analysis without the Petal.Length variable to improve the output.</p>
<pre class="r"><code>iris.pars3 &lt;- iris.pars[, -3]</code></pre>
</div>
<div id="linearity" class="section level3">
<h3>2.5. Linearity</h3>
<pre class="r"><code>psych::pairs.panels(iris[1:4], gap = 0, bg = c(&quot;red&quot;, &quot;blue&quot;, &quot;green&quot;)[iris$Species], pch = 21)</code></pre>
<p><img src="LDA_files/figure-html/unnamed-chunk-13-1.png" width="672" /></p>
<p>There are some non-linear relationships. We should try a KDA or K-mDA model instead. We will continue with our LDA model in light of example.</p>
</div>
</div>
<div id="lda-model" class="section level2">
<h2>3- LDA model</h2>
<pre class="r"><code>iris.pars3.df &lt;- as.data.frame(iris.pars3)
(iris.lda &lt;- lda(gr ~ Sepal.Length + Sepal.Width + Petal.Width, data = iris.pars3.df))</code></pre>
<pre><code>## Call:
## lda(gr ~ Sepal.Length + Sepal.Width + Petal.Width, data = iris.pars3.df)
## 
## Prior probabilities of groups:
##         1         2         3 
## 0.3333333 0.3200000 0.3466667 
## 
## Group means:
##   Sepal.Length Sepal.Width Petal.Width
## 1     5.006000    3.428000    0.246000
## 2     5.927083    2.777083    1.316667
## 3     6.571154    2.959615    2.007692
## 
## Coefficients of linear discriminants:
##                     LD1       LD2
## Sepal.Length  0.6532649 -0.659779
## Sepal.Width  -2.4718234  2.819200
## Petal.Width   4.9757063  1.464479
## 
## Proportion of trace:
##    LD1    LD2 
## 0.9896 0.0104</code></pre>
<pre class="r"><code>iris.lda$count</code></pre>
<pre><code>##  1  2  3 
## 50 48 52</code></pre>
<p>The formula of the model will be: - <span class="math inline">\(LD1 = 0.653 * Sepal.Length - 2.471 * Sepal.Width + 4.975 * Petal.Width\)</span> - <span class="math inline">\(LD2 = -0.659 * Sepal.Length + 2.819 * Sepal.Width + 1.464 * Petal.Width\)</span></p>
<p>The proporton of trace indicates that with just one LD we achive up to <strong>a 99% of discimination</strong>.</p>
<div id="plot-1" class="section level4">
<h4>Plot 1</h4>
<pre class="r"><code>ggord(iris.lda, iris$Species)</code></pre>
<p><img src="LDA_files/figure-html/unnamed-chunk-16-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<div id="plot-2" class="section level4">
<h4>Plot 2</h4>
<pre class="r"><code>partimat(factor(gr) ~ Sepal.Length + Sepal.Width + Petal.Width, data = iris.pars3.df,
         method = &quot;lda&quot;, nplots.vert = 1)</code></pre>
<p><img src="LDA_files/figure-html/unnamed-chunk-17-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>Fp &lt;- predict(iris.lda)$x
(iris.class &lt;- predict(iris.lda)$class)</code></pre>
<pre><code>##   [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1
##  [36] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 2
##  [71] 3 2 2 2 2 2 2 3 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 3 3 3 3 3
## [106] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 2 3 3 3 2 2 3 3 3 3 3
## [141] 3 3 3 3 3 3 3 3 3 3
## Levels: 1 2 3</code></pre>
</div>
<div id="plot-3" class="section level4">
<h4>Plot 3</h4>
<pre class="r"><code>par(mfrow = c(1, 1))
plot(Fp[, 1], Fp[, 2], type = &quot;n&quot;)
text(Fp[, 1], Fp[, 2], row.names(iris), col = c(as.numeric(iris.class) + 1))
abline(v = 0, lty = &quot;dotted&quot;)
abline(h = 0, lty = &quot;dotted&quot;)
for(i in 1:length(levels(as.factor(gr)))){
  cov &lt;- cov(Fp[gr == i, ])
  centre &lt;- apply(Fp[gr == i, ], 2, mean)
  lines(ellipse(cov, centre = centre, level = 0.95))
}</code></pre>
<p><img src="LDA_files/figure-html/unnamed-chunk-18-1.png" width="672" /></p>
<p>There are some problems between the variable 2 and 3 (versicolor and virginica species).</p>
</div>
</div>
<div id="canonic-discrimination-evaluation" class="section level2">
<h2>4- Canonic discrimination evaluation</h2>
<div id="canonic-value-calculation" class="section level3">
<h3>4.1. Canonic value calculation</h3>
<pre class="r"><code>iris.lda$svd ^ 2 </code></pre>
<pre><code>## [1] 1589.57124   16.66553</code></pre>
<pre class="r"><code>100 * iris.lda$svd ^ 2/ sum(iris.lda$svd ^ 2) </code></pre>
<pre><code>## [1] 98.962449  1.037551</code></pre>
<p>The first function can explain a 98% of the total variance of the sample.</p>
</div>
<div id="canonic-correlation" class="section level3">
<h3>4.2. Canonic correlation</h3>
<pre class="r"><code>punt &lt;- predict(iris.lda)$x
summary(lm(punt ~ gr))</code></pre>
<pre><code>## Response LD1 :
## 
## Call:
## lm(formula = LD1 ~ gr)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -3.0906 -0.9410 -0.0288  1.0436  3.9210 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -10.9928     0.2996  -36.69   &lt;2e-16 ***
## gr            5.4600     0.1377   39.65   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 1.391 on 148 degrees of freedom
## Multiple R-squared:  0.914,  Adjusted R-squared:  0.9134 
## F-statistic:  1572 on 1 and 148 DF,  p-value: &lt; 2.2e-16
## 
## 
## Response LD2 :
## 
## Call:
## lm(formula = LD2 ~ gr)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -2.8107 -0.6906 -0.0500  0.6202  2.8304 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)
## (Intercept)  -0.2408     0.2369  -1.017    0.311
## gr            0.1196     0.1089   1.099    0.274
## 
## Residual standard error: 1.099 on 148 degrees of freedom
## Multiple R-squared:  0.008091,   Adjusted R-squared:  0.001389 
## F-statistic: 1.207 on 1 and 148 DF,  p-value: 0.2737</code></pre>
<p>Our model can explain a 80% of the variance, so we can say our model can discrimate quite well.</p>
</div>
</div>
<div id="classification-accuracy" class="section level2">
<h2>5- Classification accuracy</h2>
<pre class="r"><code>iris.class &lt;- predict(iris.lda)$class
(iris.table &lt;- table(gr, iris.class))</code></pre>
<pre><code>##    iris.class
## gr   1  2  3
##   1 50  0  0
##   2  0 45  3
##   3  0  5 47</code></pre>
<pre class="r"><code>sum(diag(iris.table))/sum(iris.table)</code></pre>
<pre><code>## [1] 0.9466667</code></pre>
<pre class="r"><code>cohen.kappa(iris.table)</code></pre>
<pre><code>## Call: cohen.kappa1(x = x, w = w, n.obs = n.obs, alpha = alpha, levels = levels)
## 
## Cohen Kappa and Weighted Kappa correlation coefficients and confidence boundaries 
##                  lower estimate upper
## unweighted kappa  0.87     0.92  0.97
## weighted kappa    0.93     0.96  0.99
## 
##  Number of subjects = 150</code></pre>
<pre class="r"><code>cor.test(gr, as.numeric(iris.class), method = &quot;kendall&quot;)</code></pre>
<pre><code>## 
##  Kendall&#39;s rank correlation tau
## 
## data:  gr and as.numeric(iris.class)
## z = 13.001, p-value &lt; 2.2e-16
## alternative hypothesis: true tau is not equal to 0
## sample estimates:
##       tau 
## 0.9469192</code></pre>
<p>Our model could manage to classificate correctly the 94% of the data (CCR = .94). A 2% of the succesful predictions were due to chance.</p>
</div>
<div id="crossed-validation" class="section level2">
<h2>6- Crossed Validation</h2>
<pre class="r"><code>iris.lda.jac &lt;- lda(gr ~ Sepal.Length + Sepal.Width + Petal.Width, data = iris.pars3.df, CV = TRUE)
summary(iris.lda.jac)</code></pre>
<pre><code>##           Length Class  Mode   
## class     150    factor numeric
## posterior 450    -none- numeric
## terms       3    terms  call   
## call        4    -none- call   
## xlevels     0    -none- list</code></pre>
<pre class="r"><code>iris.jac.class &lt;- iris.lda.jac$class
iris.jac.table &lt;- table(gr, iris.jac.class)
diag(prop.table(iris.jac.table, 1))</code></pre>
<pre><code>##         1         2         3 
## 1.0000000 0.9375000 0.8846154</code></pre>
<pre class="r"><code>diag(prop.table(iris.table, 1))</code></pre>
<pre><code>##         1         2         3 
## 1.0000000 0.9375000 0.9038462</code></pre>
<p>The crossed model has the same success rate as the previous model. There is no improvement.</p>
</div>
<div id="prediction-example" class="section level2">
<h2>* Prediction example</h2>
<p>Let’s try to test the prediction capability of our model. For instance: Sepal.Length = 5, Sepal.Width = 3.2, Petal.Length = 1.2, Petal.Width = 0.1.</p>
<pre class="r"><code>pred &lt;- c(5, 3.2, 0.1)
pred &lt;- as.data.frame(t(pred))
colnames(pred) &lt;- colnames(iris.pars3)
(pred.result &lt;- predict(iris.lda, newdata = pred))</code></pre>
<pre><code>## $class
## [1] 1
## Levels: 1 2 3
## 
## $posterior
##   1            2            3
## 1 1 1.800572e-13 1.294227e-27
## 
## $x
##         LD1        LD2
## 1 -6.373527 -0.6513308</code></pre>
<p>This example is a sample of setosa iris (group 1).</p>
</div>
</div>
<div id="quadratic-discriminant-analysis-qda" class="section level1">
<h1>Quadratic discriminant analysis (QDA)</h1>
<p>This could be considered as the non-parametric version of the PCA. It is specially indicated with data that don’t follow a normal distribution.</p>
<pre class="r"><code>iris.qda &lt;- qda(iris[,-5], iris[,5], CV = TRUE)
(tqda &lt;- table(Original = iris$Species, Prediction = iris.qda$class))</code></pre>
<pre><code>##             Prediction
## Original     setosa versicolor virginica
##   setosa         50          0         0
##   versicolor      0         48         2
##   virginica       0          1        49</code></pre>
<pre class="r"><code>diag(prop.table(tqda, 1))</code></pre>
<pre><code>##     setosa versicolor  virginica 
##       1.00       0.96       0.98</code></pre>
<pre class="r"><code>sum(diag(tqda))/sum(tqda)</code></pre>
<pre><code>## [1] 0.98</code></pre>
<p>If we use a QDA we improve the discrimination by a 4% (CCR = .98). The non-parametric model is better since we could not accept the homocedasticity and normality assumptions.</p>
<pre class="r"><code>partimat(factor(gr) ~ Sepal.Length + Sepal.Width + Petal.Width, data = iris.pars3.df,
         method = &quot;qda&quot;, nplots.vert = 1)</code></pre>
<p><img src="LDA_files/figure-html/unnamed-chunk-25-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
