---
title: "XGBoost - Titanic"
output: html_document
---

# XGBoost
(model description)

# Model una mica mes lento que LightGBM (1.5x)
# En Random Forest qUan mes arbres menys variansa, pero hi ha limit de millora, pero XGB i LightGBM pot anar una mica mes enlla fins l'overfit. Aixo es el sweet spot.

# Mentre que els models bagging creen molts models fluixos i donen l'average de tots,
# en boosting es crea en seqüència un model weak (uns 5 nodes) rere altre. Açò té avantatges, com
# com que l'entrenament es fa a salts petits, aprenent dels errors de forma més fina
# que en models més potents, pel que tenen molt menys overfit.

# els models busquen minimitzar al màxim el MSE loss function. 

# Abans de crear model, millor passar a matriu (performance). El model requerix Hot Encode (vtreat package)

```{r libraries, message=FALSE, warning=FALSE}
# Load required libraries
library(tidyverse)
library(data.table)
library(magrittr)
library(ggridges)
library(xgboost)
library(rpart)
library(gridExtra)
library(vtreat)

# Load databases
train <- fread('C:/Users/hp/Documents/GitHub/statopia/archives/titanic_train.csv')
test <- fread('C:/Users/hp/Documents/GitHub/statopia/archives/titanic_test.csv')
```

# Exploratory Analysis
```{r}
# Let's predict survivors 
# Surviving Rate By Sex
train %>% 
  group_by(Sex) %>% 
  summarise(total = n(),
            survived = sum(Survived),
            non_survived = sum(ifelse(Survived == 1, 0, 1))) %>%
  gather(key, num, survived, non_survived) %>% 
  mutate(prop = num / total) %>% 
  ggplot(aes(x = Sex, y = num, fill = key)) +
    geom_col(position='fill') +
    geom_label(aes(label = paste(num, '-', round(prop * 100, 2), '%')),
                   position = 'fill') +
    labs(x = 'Sex', y = 'Proportion', fill = '') +
    theme_minimal() +
    theme(axis.text = element_text(size = 12),
          plot.title = element_text(size = 20),
          legend.text = element_text(size = 12)) +
    scale_y_continuous(breaks = c(0, .25, .5, .75, 1),
                       labels = c('0%', '25%', '50%', '75%', '100%')) +
    scale_fill_discrete(labels = c("Non Survived", "Survived")) +
    ggtitle('Surviving Rate by Sex')
```

```{r}
# Surviving by Fare
train %>% 
  filter(Fare < 300) %>% 
  ggplot(aes(x = Fare)) +
    geom_density(aes(fill = factor(Survived)), alpha = .5, size = 1.2) +
    theme_bw() +
    theme(axis.text = element_text(size = 12),
          plot.title = element_text(size = 20),
          legend.title = element_blank(),
          legend.text = element_text(size = 12)) +
    labs(x = 'Fare', y = 'Density') +
    scale_fill_discrete(labels = c("Non Survived", "Survived")) +
    ggtitle('Surviving Rate by Fare')
```

```{r}
# Feature  engineering
# Combining test and train datasets 
combi <- rbind(train, test)

# 1. Choose interesting variables
train2 <- train %>% 
            select(Survived, Pclass, Sex, Age = Age_wiki, Fare, SibSp, Parch)

# 2. Input missing Age using a Decision Tree
fit_age <- rpart(Age ~ SibSp + Parch + Pclass, data = train2)
train2$Age[is.na(train2$Age)] <- predict(fit_age, newdata = train2[is.na(train2$Age),])

# 3. Add a variable for family size
train2 %<>% 
  mutate(family_size = SibSp + Parch + 1) %>% 
  select(-SibSp, -Parch)

train3 <- train2 %>% 
            select(Survived, Pclass, Age, Fare, family_size) 
```

```{r}
set.seed(123)

# Create the tunning grid
hyper_grid <- expand.grid(
  eta = c(.05, .1, .2, .3),
  gamma = c(.5, 1, 1.2, 1.5),
  max_depth = c(3, 5, 7, 9),
  min_child_weight = c(1, 3, 5, 7),
  subsample = c(.5, .65, .8, 1),
  colsample_bytree = c(.7, .8, .9, 1),
  #optimal_trees = 0, 
  test_error_mean = 0
)
nrow(hyper_grid)
```
There are x possible model combinations

```{r eval=FALSE, include=FALSE}
# 3. For loop (6 hores de reproducció!)
# grid search 
for (i in 1:nrow(hyper_grid)) {
  params <- list(
    eta = hyper_grid$eta[i],
    gamma = hyper_grid$gamma[i], 
    max_depth = hyper_grid$max_depth[i],
    min_child_weight = hyper_grid$min_child_weight[i],
    subsample = hyper_grid$subsample[i],
    colsample_bytree = hyper_grid$colsample_bytree[i]
  )
  xgb.tune <- xgb.cv(
    params = params,
    data = data.matrix(train3[,-1]),
    label = train3[,1],
    nrounds = 500,
    nfold = 5,
    objective = 'binary:logistic',  # classificatory model
    verbose = 0,
    early_stopping_rounds = 10      # 
  )
  # add min training error and trees to grid
  #hyper_grid$optimal_trees[i] <- which.min(xgb.tune$evaluation_log$test_rmse_mean)
  hyper_grid$test_error_mean[i] <- min(xgb.tune$evaluation_log$test_error_mean)
}
```
That took 1 hour to run on my laptop. Now let's take a look at the best parameter combination to fit our predictive model.

```{r}
hyper_grid %>%
  dplyr::arrange(min_RMSE) %>%
  head(10)
```

```{r}
# BEst model
# Definitive model
params_final <- list(
  eta = 0.3,
  gamma = 1.2, 
  max_depth = 3, 
  min_child_weight = 1,
  subsample = 1,
  colsample_bytree = .9
)

xgb.fit <- xgb.cv(
  params = params_final,
  data = data.matrix(train3[,-1]),
  label = train3[,1],
  nrounds = 500,
  nfold = 5,
  objective = 'binary:logistic',
  #verbose = 0,
  early_stopping_rounds = 10,
  print_every_n = 100
)

xgb.fit <- xgboost(
  params = params_final,
  data = data.matrix(train3[,-1]),
  label = train3[,1],
  nrounds = 500,
  nfold = 5,
  objective = 'binary:logistic',
  #verbose = 0,
  early_stopping_rounds = 10,
  print_every_n = 100
)
# Test error: 0.24 

```


### A PARTIR D'ACI UNCHECKED

# 4. Importància dels features
importance_matrix <- xgb.importance(model = xgb.fit)
xgb.plot.importance(importance_matrix, top_n = 10, measure = 'Gain')

# 5. Predicció
pred <- predict(xgb.fit, features_test)
caret::RMSE(pred, response_test)

# Make predictions on test set for ROC curve
xgb.test.speed = predict(xgb.model.speed
                         , newdata = as.matrix(test[, colnames(test) != "Class"])
                         , ntreelimit = xgb.model.speed$bestInd)
auc.xgb.speed = roc(test$Class, xgb.test.speed, plot = TRUE, col = "blue")
print(auc.xgb.speed)
